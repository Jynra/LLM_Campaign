# LLM Campaign - Application de Jeu de RÃ´le avec IA

Cette application web permet de jouer Ã  des jeux de rÃ´le oÃ¹ un modÃ¨le de langage (LLM) tient le rÃ´le du MaÃ®tre du Jeu. Elle utilise une interface web alimentÃ©e par un serveur Python qui fait office de proxy vers des services LLM comme Ollama.

<!-- ![Roleplay avec IA](https://via.placeholder.com/800x400?text=Roleplay+avec+IA) -->

## FonctionnalitÃ©s

âœ¨ **Interface immersive** - Chat interactif pour des aventures narratives captivantes  
ğŸ² **SystÃ¨me de dÃ©s intÃ©grÃ©** - Lancez des dÃ©s virtuels pour rÃ©soudre les actions  
ğŸ¤– **IA comme MaÃ®tre du Jeu** - Utilise Ollama pour gÃ©nÃ©rer des rÃ©ponses contextuelles  
ğŸ’¾ **Sauvegarde locale** - Conservation des parties entre les sessions  
ğŸ“± **Design adaptatif** - Fonctionne sur ordinateur, tablette ou smartphone  
ğŸŒ **Mode hors-ligne** - RÃ©ponses de dÃ©monstration si Ollama n'est pas disponible

## Architecture du projet

```
LLM_Campaign/
â”œâ”€â”€ public/               # Fichiers statiques pour l'interface web
â”‚   â”œâ”€â”€ index.html        # Page principale
â”‚   â”œâ”€â”€ css/              # Styles CSS
â”‚   â”‚   â””â”€â”€ styles.css    # Feuille de style principale
â”‚   â””â”€â”€ js/               # Scripts JavaScript
â”‚       â”œâ”€â”€ config.js     # Configuration globale
â”‚       â”œâ”€â”€ models.js     # ModÃ¨les de donnÃ©es
â”‚       â”œâ”€â”€ api.js        # Communication avec Ollama
â”‚       â”œâ”€â”€ chat.js       # Gestion des conversations
â”‚       â”œâ”€â”€ ui.js         # Interface utilisateur
â”‚       â””â”€â”€ main.js       # Point d'entrÃ©e
â”œâ”€â”€ proxy.py              # Serveur HTTP Python avec proxy vers Ollama
â”œâ”€â”€ Dockerfile            # Configuration pour l'image Docker
â””â”€â”€ docker_install.sh     # Script d'installation automatisÃ©e
```

## Configuration requise

- Docker (pour l'installation avec conteneur)
- OU Python 3.x (pour l'installation locale)
- Instance Ollama fonctionnelle (optionnel, mode dÃ©mo disponible)

## Installation et dÃ©marrage

### Avec Docker (recommandÃ©)

1. Clonez le dÃ©pÃ´t et accÃ©dez au rÃ©pertoire du projet :
   ```bash
   git clone https://github.com/votre-nom/LLM_Campaign.git
   cd LLM_Campaign
   ```

2. Rendez le script d'installation exÃ©cutable :
   ```bash
   chmod +x docker_install.sh
   ```

3. ExÃ©cutez le script d'installation :
   ```bash
   ./docker_install.sh
   ```

4. AccÃ©dez Ã  l'application dans votre navigateur :
   ```
   http://localhost:9425
   ```

### Sans Docker (dÃ©veloppement local)

1. Assurez-vous que Python 3 est installÃ©

2. Installez les dÃ©pendances requises :
   ```bash
   pip install requests
   ```

3. Lancez le serveur Python :
   ```bash
   python proxy.py
   ```

4. AccÃ©dez Ã  l'application dans votre navigateur :
   ```
   http://localhost:9425
   ```

## Configuration d'Ollama

### PrÃ©requis
- Ollama installÃ© et en cours d'exÃ©cution
- Au moins un modÃ¨le de langage tÃ©lÃ©chargÃ© dans Ollama (ex: gemma3:4b, mistral, etc.)

### Configuration rÃ©seau
Par dÃ©faut, l'application est configurÃ©e pour se connecter Ã  Ollama Ã  l'adresse `http://172.17.0.8:11434`. 

Si votre instance Ollama se trouve ailleurs :
1. Modifiez la variable `OLLAMA_URL` dans le fichier `proxy.py`
2. RedÃ©marrez le serveur

## Utilisation de l'application

1. **DÃ©marrage** : Une fenÃªtre de bienvenue vous explique le fonctionnement de base
2. **SÃ©lection du personnage** : Cliquez sur un nom dans la barre latÃ©rale
3. **SÃ©lection du modÃ¨le** : Choisissez un modÃ¨le Ollama dans le menu dÃ©roulant
4. **Interaction** : Ã‰crivez vos actions dans la zone de texte et utilisez les boutons :
   - ğŸ² Pour lancer des dÃ©s
   - ğŸ—ºï¸ Pour afficher une carte (fonctionnalitÃ© future)
   - ğŸ—‘ï¸ Pour effacer la conversation
   - âš™ï¸ Pour les paramÃ¨tres (fonctionnalitÃ© future)

## Personnalisation

### Interface graphique
Modifiez les variables dans `public/css/styles.css` pour changer l'apparence :

```css
:root {
    --primary-color: #4a4e69;
    --secondary-color: #22223b;
    --accent-color: #9a8c98;
    --light-color: #f2e9e4;
    --dark-color: #22223b;
}
```

### Personnages et campagne
Personnalisez les personnages et les informations de campagne dans `public/js/ui.js` et `public/js/api.js`.

### Prompt du MaÃ®tre du Jeu
Pour ajuster le comportement du LLM comme MaÃ®tre du Jeu, modifiez la fonction `createDMPrompt` dans `public/js/api.js`.

## DÃ©pannage

### Ollama non dÃ©tectÃ©
Si Ollama n'est pas dÃ©tectÃ© :
1. VÃ©rifiez que Ollama est en cours d'exÃ©cution
2. ContrÃ´lez l'adresse configurÃ©e dans `OLLAMA_URL` (proxy.py)
3. Assurez-vous qu'au moins un modÃ¨le est installÃ© dans Ollama

L'application basculera automatiquement en mode dÃ©mo si Ollama n'est pas accessible.

### ProblÃ¨mes de proxy
Le serveur Python fait office de proxy pour Ã©viter les problÃ¨mes CORS. Si vous rencontrez des difficultÃ©s :
1. VÃ©rifiez les logs du serveur Python
2. Assurez-vous que le port 9425 n'est pas bloquÃ© par un pare-feu

## Mode dÃ©mo

Si Ollama n'est pas disponible, l'application utilise des rÃ©ponses prÃ©dÃ©finies pour dÃ©montrer ses fonctionnalitÃ©s. Ces rÃ©ponses se trouvent dans la fonction `generateDemoResponse()` du fichier `public/js/api.js`.

## DÃ©veloppement futur

- [ ] SystÃ¨me de cartes interactif
- [ ] Gestion des personnages non-joueurs
- [ ] ParamÃ¨tres de jeu configurables
- [ ] Support pour d'autres moteurs LLM
- [ ] Collaboration multi-joueurs en temps rÃ©el

## Licence

Ce projet est distribuÃ© sous licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.

## Contributions

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  soumettre des pull requests ou Ã  ouvrir des issues pour signaler des bugs ou proposer des amÃ©liorations.